import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    f1_score,
    mean_absolute_error,
    mean_squared_error,
    precision_score,
    r2_score,
    recall_score,
    roc_auc_score,
)


def show_confusion_matrix(y_test, pred):
    cm = confusion_matrix(y_test, pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()


def print_evaluation(y_test, pred, pred_proba):
    print(f"ì •í™•ë„: {accuracy_score(y_test, pred):.4f}")
    print(f"ì •ë°€ë„: {precision_score(y_test, pred):.4f}")
    print(f"ì¬í˜„ìœ¨: {recall_score(y_test, pred):.4f}")
    print(f"ì˜¤ì°¨í–‰ë ¬: {confusion_matrix(y_test, pred)}")
    print(f"F1 Score: {f1_score(y_test, pred):.4f}")
    print(f"ROC-AUC: {roc_auc_score(y_test, pred_proba):.4f}")


def evaluate_regression_model(y_test, y_pred):
    """
    íšŒê·€ ëª¨ë¸ì˜ ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ(MAE, RMSE, R2)ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print("ğŸ“Š Regression Model Evaluation")
    print("-------------------------------")
    print(f"MAE  (Average Error) : {mae:.4f}")
    print(f"MSE  (Average Error) : {mse:.4f}")
    print(f"RMSE (Large Error)   : {rmse:.4f}")
    print(f"R2 Score (Fit)      : {r2:.4f}")
    print("-------------------------------")


def show_importances(model, features):
    # 1. í•™ìŠµëœ ëª¨ë¸(ì˜ˆ: rf_reg)ì—ì„œ ì¤‘ìš”ë„ ì¶”ì¶œ
    # 'final_features'ëŠ” ì•ì„œ ìƒê´€ê³„ìˆ˜ 0.1 ì´ìƒìœ¼ë¡œ ì¶”ì¶œí•œ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    importances = model.feature_importances_
    indices = np.argsort(importances)  # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬

    # 2. ì‹œê°í™” ë°ì´í„° êµ¬ì„±
    feature_importances = pd.DataFrame(
        {
            "Feature": [features[i] for i in indices],
            "Importance": importances[indices],
        }
    )

    # 3. ê°€ë¡œ ë§‰ëŒ€ ê·¸ë˜í”„ ì‹œê°í™”
    plt.figure(figsize=(10, 8))
    sns.barplot(
        data=feature_importances,
        x="Importance",
        y="Feature",
        hue="Feature",  # yì¶• ë³€ìˆ˜ë¥¼ hueì— í• ë‹¹
        palette="magma",
        legend=False,  # ë¶ˆí•„ìš”í•œ ë²”ë¡€ ì œê±°
    )
    plt.title("Feature Importances for Delivery Time Prediction", fontsize=15)
    plt.xlabel("Importance Score")
    plt.ylabel("Features")
    plt.grid(axis="x", linestyle="--", alpha=0.7)
    plt.show()


def compare_models(models, X_train, y_train, X_test, y_test):
    results = []
    best_pred = None
    max_r2 = 0

    # 2. ë°˜ë³µë¬¸ì„ í†µí•œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    for name, model in models.items():
        model.fit(X_train, y_train)
        preds = model.predict(X_test)

        mae = mean_absolute_error(y_test, preds)
        r2 = r2_score(y_test, preds)

        if max_r2 < r2:
            max_r2 = r2
            best_pred = preds

        results.append({"Model": name, "MAE": mae, "R2 Score": r2})

    # 3. ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df_results = pd.DataFrame(results)

    # 4. ì‹œê°í™” (MAEì™€ R2 Score)
    _, ax1 = plt.subplots(figsize=(10, 6))

    # MAE ë§‰ëŒ€ ê·¸ë˜í”„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    sns.barplot(
        data=df_results, x="Model", y="MAE", hue="MAE", ax=ax1, palette="Blues_d"
    )
    ax1.set_title("Model Performance Comparison", fontsize=15)
    ax1.set_ylabel("MAE (Lower is Better)")

    # R2 Score ì„  ê·¸ë˜í”„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
    ax2 = ax1.twinx()
    sns.lineplot(
        data=df_results, x="Model", y="R2 Score", marker="o", color="red", ax=ax2
    )
    ax2.set_ylabel("R2 Score (Higher is Better)")

    plt.show()

    return best_pred


def best_model(y_test, pred):
    # ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‹œê°í™”
    plt.figure(figsize=(8, 8))
    plt.scatter(y_test, pred, alpha=0.3, color="blue")
    plt.plot(
        [y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", lw=2
    )  # ê¸°ì¤€ì„ 

    plt.xlabel("Actual Delivery Time (min)")
    plt.ylabel("Predicted Delivery Time (min)")
    plt.title("Actual vs Predicted")
    plt.show()
